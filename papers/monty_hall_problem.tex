\documentclass[11pt]{amsart}

\usepackage{amsmath, amssymb, bbm, MnSymbol, amsrefs}
\usepackage{hyperref}
\usepackage[all]{xy}
\usepackage{diagrams}
\usepackage{enumerate}
\usepackage{fancyvrb}

\setlength{\textwidth}{16cm} \setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm} \setlength{\topmargin}{0cm}
\setlength{\evensidemargin}{0cm} \setlength{\topmargin}{0cm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{dfn}[theorem]{Definition}
\newtheorem{property}[theorem]{Property}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{exercise}[theorem]{Exercise}

\newarrow{Line} -----
\newarrow{Dash}{}{dash}{}{dash}{}
\newarrow{Corresponds} <--->
\newarrow{Into} C--->
\newarrow{Embed} >--->
\newarrow{Onto} ----{>>}
\newarrow{TeXonto} ----{->>}
\newarrow{Nto} --+->
\newarrow{Dashto} {}{dash}{}{dash}>

\title{Monty Hall Problem}

\begin{document}
\maketitle

\begin{center}
Dinh Huu Nguyen, 03/10/2022
\end{center}
\vspace{20pt}

Abstract: an exposition on Monty Hall problem.

\tableofcontents

\section{The Game}
Consider the following game. A prize is placed behind door 1, door 2 or door 3 with equal probability. So if the random variable $X$ denotes where the prize is then
$$P(X = 1) = P(X = 2) = P(X = 3) = \frac{1}{3}$$

A player then chooses one of the doors with equal probability. If the door he chooses is where the prize is, he wins the prize. So if the random variable $Y$ denotes his choice then
$$P(Y = 1) = P(Y = 2) = P(Y = 3) = \frac{1}{3}$$

After the player has made his choice, the host will go behind the doors. If the prize is behind the chosen door, that is $X = Y$ then he will open one of the remaining two doors with equal probability. If the prize is not behind the chosen door, that is $X \neq Y$ then he will open the remaining door. So if the random variable $Z$ denotes his choice then
$$P(Z = 1 \,|\, X = 1, Y = 1) = 0$$
$$P(Z = 2 \,|\, X = 1, Y = 1) = \frac{1}{2}$$
$$P(Z = 3 \,|\, X = 1, Y = 1) = \frac{1}{2}$$
$$P(Z = 1 \,|\, X = 1, Y = 2) = 0$$
$$P(Z = 2 \,|\, X = 1, Y = 2) = 0$$
$$P(Z = 3 \,|\, X = 1, Y = 2) = 1$$
$$\vdots$$

Gathering all this together, we have the trivariate random variable $(X,Y,Z)$ with probability mass function $p_{X,Y,Z}(x, y, z)$ where
$$p_{X,Y,Z}(1,1,1) = 0, p_{X,Y,Z}(1,1,2) = \frac{1}{18}, p_{X,Y,Z}(1,1,3) = \frac{1}{18}$$
$$p_{X,Y,Z}(1,2,1) = 0, p_{X,Y,Z}(1,2,2) = 0, p_{X,Y,Z}(1,2,3) = \frac{1}{9}$$
$$p_{X,Y,Z}(1,3,1) = 0, p_{X,Y,Z}(1,3,2) = \frac{1}{9}, p_{X,Y,Z}(1,3,3) = 0$$
$$p_{X,Y,Z}(2,1,1) = 0, p_{X,Y,Z}(2,1,2) = 0, p_{X,Y,Z}(2,1,3) = \frac{1}{9}$$
$$p_{X,Y,Z}(2,2,1) = \frac{1}{18}, p_{X,Y,Z}(2,2,2) = 0, p_{X,Y,Z}(2,2,3) = \frac{1}{18}$$
$$p_{X,Y,Z}(2,3,1) = \frac{1}{9}, p_{X,Y,Z}(2,3,2) = 0, p_{X,Y,Z}(2,3,3) = 0$$
$$p_{X,Y,Z}(3,1,1) = 0, p_{X,Y,Z}(3,1,2) = \frac{1}{9}, p_{X,Y,Z}(3,1,3) = 0$$
$$p_{X,Y,Z}(3,2,1) = \frac{1}{9}, p_{X,Y,Z}(3,2,2) = 0, p_{X,Y,Z}(3,2,3) = 0$$
$$p_{X,Y,Z}(3,3,1) = \frac{1}{18}, p_{X,Y,Z}(3,3,2) = \frac{1}{18}, p_{X,Y,Z}(3,3,3) = 0$$

Finally, the player is given a choice to stay with his initial door or switch to the remaining door. So if the random variable $W$ denotes his switch then
$$P(W = 1 \,|\, X = 1, Y = 1, Z = 1) = 0$$
$$P(W = 2 \,|\, X = 1, Y = 1, Z = 1) = 0$$
$$P(W = 3 \,|\, X = 1, Y = 1, Z = 1) = 0$$
$$P(W = 1 \,|\, X = 1, Y = 1, Z = 2) = 0$$
$$P(W = 2 \,|\, X = 1, Y = 1, Z = 2) = 0$$
$$P(W = 3 \,|\, X = 1, Y = 1, Z = 2) = 1$$
$$\vdots$$

Gathering all this together, we have the quadvariate random variable $(X, Y, Z, W)$ with probability mass function $p_{X, Y, Z, W}(x, y, z, w)$ where
$$p_{X, Y, Z, W}(x, y, z, w) = \begin{cases} p_{X,Y,Z}(x, y, z) \text{ if } w \neq y \text{ and } w \neq z \\ 0 \text{ otherwise}\end{cases}$$

\section{Staying or Switching}
Now we can compute all probabilities in the game.

\begin{example} We can compare the probability of winning the prize by staying with the initial door against the probability of winning the prize by switching to the remaining door
\begin{align*}
P(\text{winning by staying}) & = P(X = Y) \\
 & = \sum\limits_{x = y} p_{X,Y,Z}(x, y, z) \\
 & = \frac{1}{18} + \frac{1}{18} + \frac{1}{18} + \frac{1}{18} + \frac{1}{18} + \frac{1}{18} \\
 & = \frac{1}{3}
\end{align*}
\begin{align*}
P(\text{winning by switching}) & = P(X = W) \\
 & = \sum\limits_{x = w} p_{X, Y, Z, W}(x, y, z, w) \\
 & = \sum\limits_{x \neq y \text{ and } x \neq z} p_{X,Y,Z}(x, y, z) \\
 & = \frac{1}{9} + \frac{1}{9} + \frac{1}{9} + \frac{1}{9} + \frac{1}{9} + \frac{1}{9} \\
 & = \frac{2}{3}
\end{align*}

Alternatively, one can reason that
\begin{align*}
P(X = W) & = P(X \neq Y \text{ and } X \neq Z) \\
 & = P(X \neq Y) \\
 & = 1 - P(X = Y) \\
 & = 1 - \frac{1}{3} = \frac{2}{3}
\end{align*}
\end{example}

\begin{example} We can compute the distribution of $Z$
\begin{align*}
P(Z = 1) & = \sum\limits_{z = 1} p_{X,Y,Z}(x, y, z) \\
 & = \frac{1}{18} + \frac{1}{9} + \frac{1}{9} + \frac{1}{18} \\
 & = \frac{1}{3}
\end{align*}
Similarly, $P(Z = 2) = P(Z = 3) = \frac{1}{3}$.
\end{example}

\begin{example} We can compute the distribution of $W$
\begin{align*}
P(W = 1) & = \sum\limits_{w = 1} p_{X, Y, Z, W}(x, y, z, w) \\
 & = \sum\limits_{y \neq 1 \text{ and } z \neq 1} p_{X,Y,Z}(x, y, z) \\
 & = \frac{1}{9} + \frac{1}{9} + \frac{1}{18} + \frac{1}{18} \\
 & = \frac{1}{3}
\end{align*}
Similarly, $P(W = 2) = P(W = 3) = \frac{1}{3}$.
\end{example}

All $X, Y, Z, W$ are identically distributed.

\section{Information and Entropy} If we think the host's action makes a difference in this game, can we quantify it? That brings up information and entropy. We limit ourselves to discrete random variables.
\dfn For each random variable $(\Omega, \mathcal{F}, P) \rTo^X (\mathbb{R}, \mathcal{B}(\mathbb{R}))$, we define its information to be the random variable
\begin{align*}
(\mathbb{R}, \mathcal{B}(\mathbb{R}), X_*(P)) & \rTo^{I_X} (\mathbb{R}, \mathcal{B}(\mathbb{R})) \\
x & \mapsto \begin{cases} 0 \text{ if } p_X(x) = 0 \\ \log_2 \left( \frac{1}{p_X(x)} \right) \text{ otherwise} \end{cases}
\end{align*}

Note that $p_X(x) = X_*(P)(x)$. Here the base is chosen to be 2, and the unit of $I_X$ is called {\it bit}. Other popular bases are $10$ with unit {\it nat} and $e$ with unit {\it hartley}. All results then differ by constant scalars $\log_{10} 2$ and $\log_e 2$ respectively.

\begin{example}\label{jointinformation} The joint random variable $(X,Y)$ has information
\begin{align*}
(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2), (X,Y)_*(P)) & \rTo^{I_{X,Y}} (\mathbb{R}, \mathcal{B}(\mathbb{R})) \\
(x, y) & \mapsto \begin{cases} 0 \text{ if } p_{X,Y}(x, y) = 0 \\ \log_2 \left( \frac{1}{p_{X,Y}(x, y)} \right) \text{ otherwise} \end{cases}
\end{align*}

Note that $p_{X,Y}(x, y) = (X,Y)_*(P)(x, y)$. We will use this in example \ref{jointentropyexample}.
\end{example}

\dfn For each random variable $X$, we define its entropy $H(X)$ to be the expected value $E(I_X)$ of $I_X$.

Explicitly, one has
\begin{align*}
H(X) & = \sum\limits_{x_i} p_X(x_i)I_X(x_i) \\
 & = \sum\limits_{x_i} p_X(x_i) \log_2 \left(\frac{1}{p_X(x_i)} \right)
\end{align*}

For each pair of random variables $X, Y$, given event $Y = y$, we can think of
\begin{itemize}
\item conditional probability measure $P(- \,|\, Y = y)$
\item its pushforward $X_*(P(- \,|\, Y = y))$
\item random variable $(\mathbb{R}, \mathcal{B}(\mathbb{R}), X_*(P(- \,|\, Y = y))) \rTo^{X \,|\, Y = y} (\mathbb{R}, \mathcal{B}(\mathbb{R}))$
\item its information $I_{X \,|\, Y = y}$
\end{itemize}

Note that $p_{X \,|\, Y = y}(x) = X_*(P(- \,|\, Y = y))(x)$.

\dfn For each pair of random variables $X,Y$, we define the entropy $H(X \,|\, Y = y)$ of $X$ given an event $Y = y$ to be the expected value $E(I_{X \,|\, Y = y})$ of $I_{X \,|\, Y = y}$.

Explicitly, one has
\begin{align*}
H(X \,|\, Y = y) & = \sum\limits_{x_i} p_{X \,|\, Y = y}(x_i) I_{X \,|\, Y = y}(x_i) \\
 & = \sum\limits_{x_i} p_{X \,|\, Y = y}(x_i) \log_2 \left( \frac{1}{p_{X \,|\, Y = y}(x_i)} \right)
\end{align*}

If we view $(\mathbb{R}, \mathcal{B}(\mathbb{R}), Y_*(P)) \rTo^{H(X \,|\, Y = -)} (\mathbb{R}, \mathcal{B}(\mathbb{R})), y \mapsto H(X \,|\, Y = y)$ as a random variable then it has expected value
\begin{align*}
E(H(X \,|\, Y = -)) & = \sum\limits_{y_j} p_Y(y_j) H(X \,|\, Y = y_j) \\
 & = \sum\limits_{y_j} p_Y(y_j) \sum\limits_{x_i} p_{X \,|\, Y = y_j}(x_i) \log_2 \left(\frac{1}{p_{X \,|\, Y = y_j}(x_i)} \right) \\
 & = \sum\limits_{x_i, y_j} p_Y(y_j) p_{X \,|\, Y = y_j}(x_i)  \log_2 \left( \frac{1}{p_{X \,|\, Y = y_j}(x_i)} \right) \\
 & = \sum\limits_{x_i, y_j} p_Y(y_j) \frac{p_{X,Y}(x_i, y_j)}{p_Y(y_j)} \log_2 \left( \frac{1}{p_{X \,|\, Y = y_j}(x_i)} \right) \\
 & = \sum\limits_{x_i, y_j} p_{X,Y}(x_i, y_j) \log_2 \left( \frac{p_Y(y_j)}{p_{X,Y}(x_i, y_j)} \right) \\
\end{align*}

\dfn\label{conditionalentropy} For each pair of random variables $X, Y$ we define the entropy $H(X \,|\, Y)$ of $X$ given $Y$ to be the expected value $E(H(X \,|\, Y = -))$ of $H(X \,|\, Y = -)$ above.

The difference $H(X) - H(X \,|\, Y)$ is called mutual information between $X$ and $Y$ and denoted by $I(X,Y)$. It also equals the mutual information $I(Y,X) = H(Y) - H(Y \,|\, X)$ between $Y$ and $X$ and has explicit formula
$$I(X,Y) = \sum\limits_{i,j} p_{X,Y}(x_i, y_j) \log_2 \left( \frac{p_{X,Y}(x_i, y_j)}{p_X(x_i)p_Y(y_j)} \right)$$

Now we can compute all informations and entropies in the game.
\begin{example}\label{} Without the host's action, the prize location $X$ has entropy
\begin{align*}
H(X) & = \frac{1}{3} \log_2 \left( \frac{1}{\frac{1}{3}} \right) + \frac{1}{3} \log_2 \left( \frac{1}{\frac{1}{3}} \right) + \frac{1}{3} \log_2  \left( \frac{1}{\frac{1}{3}} \right) \\
 & = \log_2 3
\end{align*}
\end{example}

\begin{example}\label{} With the host's action of opening door 3, the prize location $X \,|\, Z = 3$ has entropy
\begin{align*}
H(X \,|\, Z = 3) & = p_{X \,|\, Z = 3}(1)\log_2 \left(\frac{1}{p_{X \,|\, Z = 3}(1)} \right) + p_{X \,|\, Z = 3}(2)\log_2 \left(\frac{1}{p_{X \,|\, Z = 3}(2)} \right) + p_{X \,|\, Z = 3}(3) \log_2 \left( \frac{1}{p_{X \,|\, Z = 3}(3)} \right) \\
 & = \frac{1}{2} \log_2 \left( \frac{1}{\frac{1}{2}} \right) + \frac{1}{2} \log_2 \left( \frac{1}{\frac{1}{2}} \right) + 0 \\
 & = 1
\end{align*}

With the host's action of opening door 3, the entropy of $X$ decreases. Similarly $H(X \,|\, Z = 1) = H(X \,|\, Z = 2) = 1$.
\end{example}

\begin{example} With the host's action, the prize location has expected entropy
\begin{align*}
H(X \,|\, Z) & = p_Z(1)H(X \,|\, Z = 1) + p_Z(2)H(X \,|\, Z = 2) + p_Z(3)H(X \,|\, Z = 3) \\
 & = \frac{1}{3} \cdot 1 + \frac{1}{3} \cdot 1 + \frac{1}{3} \cdot 1 \\
 & = 1
\end{align*}

One can verify this directly with the explicit formula in definition \ref{conditionalentropy} as well.
\end{example}

\begin{example}\label{} The information the host provides about the prize location is
\begin{align*}
I(X, Z) & = 0 + (\frac{1}{18} + 0 + \frac{1}{9}) \log_2 \left(\frac{\frac{1}{18} + 0 + \frac{1}{9}}{\frac{1}{3} \cdot \frac{1}{3}} \right) + (\frac{1}{18} + \frac{1}{9} + 0) \log_2 \left(\frac{\frac{1}{18} + \frac{1}{9} + 0}{\frac{1}{3} \cdot \frac{1}{3}} \right) \\
 & + (0 + \frac{1}{18} + \frac{1}{9}) \log_2 \left(\frac{0 + \frac{1}{18} + \frac{1}{9}}{\frac{1}{3} \cdot \frac{1}{3}} \right) + 0 + (\frac{1}{9} + \frac{1}{18} + 0) \log_2 \left(\frac{\frac{1}{9} + \frac{1}{18} + 0}{\frac{1}{3} \cdot \frac{1}{3}} \right) \\
 & (0 + \frac{1}{9} + \frac{1}{18}) \log_2 \left(\frac{0 + \frac{1}{9} + \frac{1}{18}}{\frac{1}{3} \cdot \frac{1}{3}} \right) + (\frac{1}{9} + 0 + \frac{1}{18}) \log_2 \left(\frac{\frac{1}{9} + 0 + \frac{1}{18}}{\frac{1}{3} \cdot \frac{1}{3}} \right) + 0 \\
 & = \log_2 \frac{3}{2} \\
 & = \log_2 3 - 1 \\
\end{align*}

One can see that $I(X,Z) = H(X) - H(X \,|\, Z)$. This is how much the entropy of $X$ decreases by.
\end{example}

\begin{example}\label{jointentropyexample} The joint random variable $(X,Z)$ has entropy
\begin{align*}
H(X,Z) & = E(I_{X,Z}) \\
 & = \sum\limits_{x_i, z_k} p_{X,Z}(x_i, z_k) \log_2 \left(\frac{1}{p_{X,Z}(x_i, z_k)} \right) \\
 & = \frac{1}{6} \log_2 \left(\frac{1}{\frac{1}{6}} \right) + \frac{1}{6} \log_2 \left(\frac{1}{\frac{1}{6}} \right) + \frac{1}{6} \log_2 \left(\frac{1}{\frac{1}{6}} \right) + \frac{1}{6} \log_2 \left(\frac{1}{\frac{1}{6}} \right) + \frac{1}{6} \log_2 \left(\frac{1}{\frac{1}{6}} \right) + \frac{1}{6} \log_2 \left(\frac{1}{\frac{1}{6}} \right) \\
 & = \log_2 6 \\
 & = \log_2 3 + 1
\end{align*}

One can see that $H(X,Z) = H(X) + H(Z) - I(X,Z) = H(X \,|\, Z) + I(X,Z) + H(Z \,|\, X)$. Draw a Venn diagram.
\end{example}

\begin{exercise}\label{} Compute $H(Y), H(Y \,|\, Z = 3), H(Y \,|\, Z), I(Y, Z), H(Y, Z)$. Hint: swap prize and player.
\end{exercise}
\section{Simulation} See Python code and games at \href{https://github.com/dinhuun/probability_statistics}{github.com/dinhuun/probability\_statistics}.

\end{document}
